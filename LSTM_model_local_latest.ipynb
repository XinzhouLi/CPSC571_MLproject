{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XinzhouLi/Toxic_Language_Detection_in_Social_Media/blob/main/LSTM_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgQz6LomhSUY",
        "outputId": "3cf06aec-bb73-4a2e-d558-90110863cfaa"
      },
      "outputs": [],
      "source": [
        "# ! git clone https://github.com/XinzhouLi/Toxic_Language_Detection_in_Social_Media.git\n",
        "# ! pip install torch==1.8 torchtext==0.9"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4TOjKmw8hSUd"
      },
      "source": [
        " https://www.cnblogs.com/cxq1126/p/13466998.html\n",
        " 这篇文章 有讲述如何怎么建立完整的训练过程\n",
        "\n",
        "\n",
        " https://www.cnblogs.com/cxq1126/p/13504437.html\n",
        " 这个就是我们的最终目标 LSTM加上Attention\n",
        "\n",
        "\n",
        " https://www.cnblogs.com/cxq1126/p/13508696.html\n",
        " 这个是transformer的文本分类我还没看先扔这\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yr7kCVmDXkwX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\progarmSoftware\\Miniconda\\envs\\3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "from torch import nn, optim\n",
        "import torchtext\n",
        "import re\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txQohGaDhSUf",
        "outputId": "ac128df0-e54e-4109-e13f-13c359dc06f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.8.2\n",
            "0.9.2\n",
            "11.1\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "# print(sys.version)\n",
        "print(torchtext.__version__)\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zcGlcRhFhSUh"
      },
      "outputs": [],
      "source": [
        "def unify_format(text):\n",
        "    return re.sub(r\"[^a-zA-Z0-9]\", \" \", text).lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VNdvyUrghSUh"
      },
      "outputs": [],
      "source": [
        "# implement dataset extends util.data.Dataset\n",
        "class ToxicDataset(data.Dataset):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        # Read data in\n",
        "        if len(args) == 1:\n",
        "            # for initialize training dataset\n",
        "            filepath = args[0]\n",
        "            self.dataframe = pd.read_csv(\n",
        "                filepath, iterator=True, header=0, encoding='utf-8', usecols=['comment_text', 'toxic'])\n",
        "            # initialize pandas dataframe to store the data\n",
        "            self.dataframe = pd.DataFrame(self.dataframe.read())\n",
        "            self.dataframe.convert_dtypes()\n",
        "            # Clean the data using regular expersion, only reserve letter and number\n",
        "            self.dataframe['comment_text'] = self.dataframe['comment_text'].apply(unify_format)\n",
        "            self.dataframe.to_csv('train_.csv', index=False, header=[\"text\", \"label\"])\n",
        "        else:\n",
        "            # for initialize testing dataset\n",
        "            filepath1 = args[0]\n",
        "            filepath2 = args[1]\n",
        "            test_comments = pd.DataFrame(pd.read_csv(\n",
        "                filepath1, iterator=True, header=0, usecols=['comment_text', 'id']).read())\n",
        "            test_label = pd.DataFrame(pd.read_csv(\n",
        "                filepath2, iterator=True, header=0, usecols=['id', 'toxic']).read())\n",
        "            # merge two dataframe together by unique key id\n",
        "            result = pd.merge(test_comments, test_label, how='left', on=['id'])\n",
        "            # select the text and label col\n",
        "            self.dataframe = result[result['toxic']>= 0].loc[:, ['comment_text', 'toxic']]\n",
        "            self.dataframe.convert_dtypes()\n",
        "            # Clean the data using regular expersion, only reserve letter and number\n",
        "            self.dataframe['comment_text'] = self.dataframe['comment_text'].apply(unify_format)\n",
        "            self.dataframe.to_csv('test_.csv', index=False, header=[\"text\", \"label\"])\n",
        "            \n",
        "\n",
        "    # override the getiem function, return tuple contains comment and label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment, label = self.dataframe.iat[index, 0], self.dataframe.iat[index, 1]\n",
        "        return (comment, label)\n",
        "\n",
        "    # Override the len function, return the length of the dataframe\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe.loc[:, ['comment_text']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UaTsO5PBhSUj"
      },
      "outputs": [],
      "source": [
        "# init dataset\n",
        "# train_dataset = ToxicDataset(\"train.csv\")\n",
        "# test_dataset = ToxicDataset(\"test.csv\", \"test_labels.csv\")\n",
        "# train_dataset = ToxicDataset(\"/content/Toxic_Language_Detection_in_Social_Media/train.csv\")\n",
        "# test_dataset = ToxicDataset(\"/content/Toxic_Language_Detection_in_Social_Media/test.csv\", \"/content/Toxic_Language_Detection_in_Social_Media/test_labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_dataset(csv_data, text_field, label_field, test=False):\n",
        "    fields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
        "                 (\"text\", text_field), (\"label\", label_field)]       \n",
        "    examples = []\n",
        "\n",
        "    if test:\n",
        "        # 如果为测试集，则不加载label\n",
        "        for text in csv_data['text']:\n",
        "            examples.append(torchtext.legacy.data.Example.fromlist([None, text, None], fields))\n",
        "    else:\n",
        "        for text, label in zip(csv_data['text'], csv_data['label']):\n",
        "            examples.append(torchtext.legacy.data.Example.fromlist([None, text, label], fields))\n",
        "    return examples, fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKbhrSFlGSuj",
        "outputId": "d0ceb609-cf83-436e-e001-ac70f36477b4"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train_.csv')\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "tokenize = lambda x: x.split()\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenize, lower=True)\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# 得到构建Dataset所需的examples和fields\n",
        "train_examples, train_fields = get_dataset(train_data, TEXT, LABEL)\n",
        "test_examples, test_fields = get_dataset(train_data, TEXT, LABEL)\n",
        "# 构建Dataset数据集\n",
        "train = torchtext.legacy.data.Dataset(train_examples, train_fields)\n",
        "test = torchtext.legacy.data.Dataset(test_examples, test_fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['text', 'label'])\n",
            "['explanation', 'why', 'the', 'edits', 'made', 'under', 'my', 'username', 'hardcore', 'metallica', 'fan', 'were', 'reverted', 'they', 'weren', 't', 'vandalisms', 'just', 'closure', 'on', 'some', 'gas', 'after', 'i', 'voted', 'at', 'new', 'york', 'dolls', 'fac', 'and', 'please', 'don', 't', 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', 'i', 'm', 'retired', 'now', '89', '205', '38', '27']\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(train[0].__dict__.keys())\n",
        "print(train[0].text)\n",
        "print(train[6].label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEXT.build_vocab(train, max_size=10000, vectors='glove.6B.300d',)\n",
        "LABEL.build_vocab(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torchtext.legacy.data.field.Field object at 0x000002B5C94349D0>\n"
          ]
        }
      ],
      "source": [
        "print(TEXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DX2dghJehSUl"
      },
      "outputs": [],
      "source": [
        "class BLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embeding_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # implement embedding function to convert text to numerical data\n",
        "        self.embedding = nn.Embedding(vocab_size, embeding_dim)\n",
        "        self.rnn = nn.LSTM(embeding_dim, hidden_dim, num_layers =2, bidirectional=True, dropout = 0.5)\n",
        "        self.fc = nn.Linear(hidden_dim*2,1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    def forward(self, x):\n",
        "\n",
        "        embediding = self.dropout(self.embedding(x))\n",
        "        output,(hidden, cell) = self.rnn(embediding)\n",
        "        hidden = torch.cat([hidden[-2], hidden[-1]],dim=1)\n",
        "        hidden = self.dropout(hidden)\n",
        "        out = self.fc(hidden)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = torchtext.legacy.data.BucketIterator(dataset=train, batch_size=180, shuffle=True)#, sort_within_batch=False, sort_key=lambda x: len(x.text), repeat=False)\n",
        "test_dataloader = torchtext.legacy.data.BucketIterator(dataset=test, batch_size=180, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pchnepb4hSUn",
        "outputId": "ce416f07-c56e-44a1-90a2-b02af785e472"
      },
      "outputs": [],
      "source": [
        "# init Lstm model \n",
        "BLSTM = BLSTMModel(len(TEXT.vocab), 300, 256)\n",
        "# replace the embedding to the pretrained embedding Glove\n",
        "pretrained = TEXT.vocab.vectors\n",
        "BLSTM.embedding.weight.data.copy_(pretrained) \n",
        "\n",
        "optimizer = optim.Adam(BLSTM.parameters(), lr=1e-3)\n",
        "criteon = nn.BCEWithLogitsLoss().to(DEVICE)\n",
        "BLSTM = BLSTM.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "syhrAX86hSUo",
        "outputId": "44f477da-ed82-4aa7-9981-a88cfac71a04"
      },
      "outputs": [],
      "source": [
        "# training process\n",
        "# 没写完\n",
        "def binary_acc(preds, y):\n",
        "    \"\"\"\n",
        "    get accuracy\n",
        "    \"\"\"\n",
        "    preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = torch.eq(preds, y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def train(rnn, dataloader, optimizer, criteon):\n",
        "    \n",
        "    avg_acc = []\n",
        "    rnn.train()\n",
        "    i=0\n",
        "    for data in dataloader:\n",
        "        text = data.text\n",
        "        text = text.to(DEVICE)\n",
        "        label = data.label\n",
        "        label = label.to(DEVICE)\n",
        "        # [seq, b] => [b, 1] => [b]\n",
        "\n",
        "        pred = rnn(text).squeeze(1)\n",
        "        # \n",
        "        loss = criteon(pred.float(), label.float())\n",
        "        acc = binary_acc(pred, label).item()\n",
        "        avg_acc.append(acc)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i%500 == 0:\n",
        "            print(i, acc)\n",
        "        i +=1\n",
        "        \n",
        "    avg_acc = np.array(avg_acc).mean()\n",
        "    print('avg acc:', avg_acc)\t\t\n",
        "    \n",
        "def eval(rnn, iterator, criteon):\n",
        "    \n",
        "    avg_acc = []\n",
        "    \n",
        "    rnn.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in iterator:\n",
        "            text = data.text\n",
        "            text = text.to(DEVICE)\n",
        "            label = data.label\n",
        "            label = label.to(DEVICE)\n",
        "            # [b, 1] => [b]\n",
        "            pred = rnn(text).squeeze(1)\n",
        "\n",
        "            #\n",
        "            loss = criteon(pred.float(), label.float())\n",
        "\n",
        "            acc = binary_acc(pred, label).item()\n",
        "            avg_acc.append(acc)\n",
        "        \n",
        "    avg_acc = np.array(avg_acc).mean()\n",
        "    \n",
        "    print('>>test:', avg_acc)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0.2888889014720917\n",
            "500 0.9611111283302307\n",
            "avg acc: 0.9493802320527116\n",
            "0 0.9722222685813904\n",
            "500 0.9555555582046509\n",
            "avg acc: 0.963284778890642\n",
            "0 0.9777777791023254\n",
            "500 0.9722222685813904\n",
            "avg acc: 0.9667297369194676\n",
            "0 0.9611111283302307\n",
            "500 0.9611111283302307\n",
            "avg acc: 0.9685966161488143\n",
            "0 0.9833333492279053\n",
            "500 0.9611111283302307\n",
            "avg acc: 0.9704122875293185\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(5):\n",
        "    train(BLSTM, train_dataloader, optimizer, criteon)\n",
        "torch.save(BLSTM.state_dict(), \"Model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>test: 0.9744206670306312\n"
          ]
        }
      ],
      "source": [
        "BLSTM.load_state_dict(torch.load(\"Model.pth\"))\n",
        "eval(BLSTM, test_dataloader, criteon)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "latest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "db41d3582c08adb439fa2713759a627d99be87d9d230168112799ce822bba7fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
