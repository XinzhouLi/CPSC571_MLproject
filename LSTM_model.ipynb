{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XinzhouLi/Toxic_Language_Detection_in_Social_Media/blob/main/LSTM_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! git clone https: // github.com/XinzhouLi/Toxic_Language_Detection_in_Social_Media.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yr7kCVmDXkwX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils import data\n",
        "from torch import nn, optim\n",
        "import torchtext\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unify_format(text):\n",
        "    return re.sub(r\"[^a-zA-Z0-9]\", \" \", text).lower().split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# implement dataset extends util.data.Dataset\n",
        "class ToxicDataset(data.Dataset):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        # Read data in\n",
        "        if len(args) == 1:\n",
        "            # for initialize training dataset\n",
        "            filepath = args[0]\n",
        "            self.dataframe = pd.read_csv(\n",
        "                filepath, iterator=True, header=0, encoding='utf-8', usecols=['comment_text', 'toxic'])\n",
        "            # initialize pandas dataframe to store the data\n",
        "            self.dataframe = pd.DataFrame(self.dataframe.read())\n",
        "            self.dataframe.convert_dtypes()\n",
        "            # Clean the data using regular expersion, only reserve letter and number\n",
        "            self.dataframe['comment_text'] = self.dataframe['comment_text'].apply(\n",
        "                unify_format)\n",
        "        else:\n",
        "            # for initialize testing dataset\n",
        "            filepath1 = args[0]\n",
        "            filepath2 = args[1]\n",
        "            test_comments = pd.DataFrame(pd.read_csv(\n",
        "                filepath1, iterator=True, header=0, usecols=['comment_text', 'id']).read())\n",
        "            test_label = pd.DataFrame(pd.read_csv(\n",
        "                filepath2, iterator=True, header=0, usecols=['id', 'toxic']).read())\n",
        "            # merge two dataframe together by unique key id\n",
        "            result = pd.merge(test_comments, test_label, how='left', on=['id'])\n",
        "            # select the text and label col\n",
        "            self.dataframe = result[result['toxic']>= 0].loc[:, ['comment_text', 'toxic']]\n",
        "            self.dataframe.convert_dtypes()\n",
        "            # Clean the data using regular expersion, only reserve letter and number\n",
        "            self.dataframe['comment_text'] = self.dataframe['comment_text'].apply(unify_format)\n",
        "\n",
        "    # override the getiem function, return tuple contains comment and label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment, label = self.dataframe.iat[index, 0], self.dataframe.iat[index, 1]\n",
        "        return comment, label\n",
        "\n",
        "    # Override the len function, return the length of the dataframe\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe.loc[:, ['comment_text']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# init dataset\n",
        "train_dataset = ToxicDataset(\"train.csv\")\n",
        "test_dataset = ToxicDataset(\"test.csv\", \"test_labels.csv\")\n",
        "# init dataloader\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=100, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(['including', 'some', 'appropriate', 'mention', 'of', 'the', 'solomon', 'article', 'is', 'not', 'without', 'some', 'level', 'of', 'support'], 0)\n",
            "0.14.1\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2635220400 bytes.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32md:\\All kind of WorkingSpaces\\Toxic_Language_Detection_in_Social_Media\\LSTM_model.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/All%20kind%20of%20WorkingSpaces/Toxic_Language_Detection_in_Social_Media/LSTM_model.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(train_dataset\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(\u001b[39m110\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/All%20kind%20of%20WorkingSpaces/Toxic_Language_Detection_in_Social_Media/LSTM_model.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(torchtext\u001b[39m.\u001b[39m__version__)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/All%20kind%20of%20WorkingSpaces/Toxic_Language_Detection_in_Social_Media/LSTM_model.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m text \u001b[39m=\u001b[39m torchtext\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mGloVe()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/All%20kind%20of%20WorkingSpaces/Toxic_Language_Detection_in_Social_Media/LSTM_model.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(text\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape())\n",
            "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torchtext\\vocab\\vectors.py:220\u001b[0m, in \u001b[0;36mGloVe.__init__\u001b[1;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl[name]\n\u001b[0;32m    219\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mglove.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39md.txt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, \u001b[39mstr\u001b[39m(dim))\n\u001b[1;32m--> 220\u001b[0m \u001b[39msuper\u001b[39m(GloVe, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(name, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torchtext\\vocab\\vectors.py:59\u001b[0m, in \u001b[0;36mVectors.__init__\u001b[1;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munk_init \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor\u001b[39m.\u001b[39mzero_ \u001b[39mif\u001b[39;00m unk_init \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m unk_init\n\u001b[1;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache(name, cache, url\u001b[39m=\u001b[39;49murl, max_vectors\u001b[39m=\u001b[39;49mmax_vectors)\n",
            "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torchtext\\vocab\\vectors.py:170\u001b[0m, in \u001b[0;36mVectors.cache\u001b[1;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mLoading vectors from \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path_pt))\n\u001b[1;32m--> 170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitos, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstoi, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectors, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(path_pt)\n",
            "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:789\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    787\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    788\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 789\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    790\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[0;32m    791\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:1131\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1130\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1131\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1133\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1135\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:1101\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[0;32m   1100\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1101\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1103\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
            "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:1079\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[0;32m   1077\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m-> 1079\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39muntyped()\n\u001b[0;32m   1080\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m   1083\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1084\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2635220400 bytes."
          ]
        }
      ],
      "source": [
        "# Testing code \n",
        "print(train_dataset.__getitem__(110))\n",
        "print(torchtext.__version__)\n",
        "text = torchtext.vocab.GloVe()\n",
        "print(text.vectors.shape())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, vocab_size, embeding_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # implement embedding function to convert text to numerical data\n",
        "        self.embedding = nn.Embedding(vocab_size, embeding_dim)\n",
        "        self.rnn = nn.LSTM(embeding_dim, hidden_dim, num_layer =2, bidirectional=True, dropout = 0.5)\n",
        "        self.fc = nn.Linear(hidden_dim*2,1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    def forward(self, x):\n",
        "        embediding = self.dropout(self.embedding(x))\n",
        "        output,(hidden, cell) = self.rnn(embediding)\n",
        "        hidden = torch.cat([hidden[-2], hidden[-1]],dim=1)\n",
        "        hidden = self.dropout(hidden)\n",
        "        out = self.fc(hidden)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "iAt based indexing can only have integer indexers",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32md:\\All kind of WorkingSpaces\\Toxic_Language_Detection_in_Social_Media\\LSTM_model.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/All%20kind%20of%20WorkingSpaces/Toxic_Language_Detection_in_Social_Media/LSTM_model.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/All%20kind%20of%20WorkingSpaces/Toxic_Language_Detection_in_Social_Media/LSTM_model.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m embeding_size \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/All%20kind%20of%20WorkingSpaces/Toxic_Language_Detection_in_Social_Media/LSTM_model.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(train_dataset[\u001b[39m'\u001b[39;49m\u001b[39mcomment_text\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39munique())\n",
            "\u001b[1;32md:\\All kind of WorkingSpaces\\Toxic_Language_Detection_in_Social_Media\\LSTM_model.ipynb Cell 9\u001b[0m in \u001b[0;36mToxicDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/All%20kind%20of%20WorkingSpaces/Toxic_Language_Detection_in_Social_Media/LSTM_model.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/All%20kind%20of%20WorkingSpaces/Toxic_Language_Detection_in_Social_Media/LSTM_model.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     comment, label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataframe\u001b[39m.\u001b[39;49miat[index, \u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataframe\u001b[39m.\u001b[39miat[index, \u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/All%20kind%20of%20WorkingSpaces/Toxic_Language_Detection_in_Social_Media/LSTM_model.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m comment, label\n",
            "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:2220\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2217\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2218\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid call for scalar access (getting)!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2220\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_key(key)\n\u001b[0;32m   2221\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n",
            "File \u001b[1;32md:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:2294\u001b[0m, in \u001b[0;36m_iAtIndexer._convert_key\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2292\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m key:\n\u001b[0;32m   2293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_integer(i):\n\u001b[1;32m-> 2294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39miAt based indexing can only have integer indexers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2295\u001b[0m \u001b[39mreturn\u001b[39;00m key\n",
            "\u001b[1;31mValueError\u001b[0m: iAt based indexing can only have integer indexers"
          ]
        }
      ],
      "source": [
        "vocab_size = 10000\n",
        "embeding_size = 300\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOMdefZJbtrcS+AakPmNnpu",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
