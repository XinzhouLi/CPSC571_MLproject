{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XinzhouLi/Toxic_Language_Detection_in_Social_Media/blob/main/LSTM_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! git clone https://github.com/XinzhouLi/Toxic_Language_Detection_in_Social_Media.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " https://www.cnblogs.com/cxq1126/p/13466998.html\n",
        " 这篇文章 有讲述如何怎么建立完整的训练过程\n",
        "\n",
        "\n",
        " https://www.cnblogs.com/cxq1126/p/13504437.html\n",
        " 这个就是我们的最终目标 LSTM加上Attention\n",
        "\n",
        "\n",
        " https://www.cnblogs.com/cxq1126/p/13508696.html\n",
        " 这个是transformer的文本分类我还没看先扔这\n",
        "\n",
        "\n",
        "我相信都接触过colab吧， 因为需要大量运算基本都不在本地跑，全都白嫖Google的算力， \n",
        "然后上面那一行是先把文件克隆到自己的drive里面这样就省的自己上传文件了\n",
        "然后你们可以先看看， 我基本都写了comment， 有一些没写的十有八九我也是抄的\n",
        "所以我也不一定知道那些代码是干嘛的， 也试试我的那些variable里面都存了啥， 每一行改一改试试，看看都是干嘛的。等之后见面开会再一起写写\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yr7kCVmDXkwX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils import data\n",
        "from torch import nn, optim\n",
        "import torchtext\n",
        "import re\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.1\n",
            "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\n",
            "0.14.1\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(sys.version)\n",
        "print(torchtext.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unify_format(text):\n",
        "    return re.sub(r\"[^a-zA-Z0-9]\", \" \", text).lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# implement dataset extends util.data.Dataset\n",
        "class ToxicDataset(data.Dataset):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        # Read data in\n",
        "        if len(args) == 1:\n",
        "            # for initialize training dataset\n",
        "            filepath = args[0]\n",
        "            self.dataframe = pd.read_csv(\n",
        "                filepath, iterator=True, header=0, encoding='utf-8', usecols=['comment_text', 'toxic'])\n",
        "            # initialize pandas dataframe to store the data\n",
        "            self.dataframe = pd.DataFrame(self.dataframe.read())\n",
        "            self.dataframe.convert_dtypes()\n",
        "            # Clean the data using regular expersion, only reserve letter and number\n",
        "            self.dataframe['comment_text'] = self.dataframe['comment_text'].apply(\n",
        "                unify_format)\n",
        "        else:\n",
        "            # for initialize testing dataset\n",
        "            filepath1 = args[0]\n",
        "            filepath2 = args[1]\n",
        "            test_comments = pd.DataFrame(pd.read_csv(\n",
        "                filepath1, iterator=True, header=0, usecols=['comment_text', 'id']).read())\n",
        "            test_label = pd.DataFrame(pd.read_csv(\n",
        "                filepath2, iterator=True, header=0, usecols=['id', 'toxic']).read())\n",
        "            # merge two dataframe together by unique key id\n",
        "            result = pd.merge(test_comments, test_label, how='left', on=['id'])\n",
        "            # select the text and label col\n",
        "            self.dataframe = result[result['toxic']>= 0].loc[:, ['comment_text', 'toxic']]\n",
        "            self.dataframe.convert_dtypes()\n",
        "            # Clean the data using regular expersion, only reserve letter and number\n",
        "            self.dataframe['comment_text'] = self.dataframe['comment_text'].apply(unify_format)\n",
        "\n",
        "    # override the getiem function, return tuple contains comment and label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment, label = self.dataframe.iat[index, 0], self.dataframe.iat[index, 1]\n",
        "        return comment, label\n",
        "\n",
        "    # Override the len function, return the length of the dataframe\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe.loc[:, ['comment_text']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# init dataset\n",
        "train_dataset = ToxicDataset(\"train.csv\")\n",
        "test_dataset = ToxicDataset(\"test.csv\", \"test_labels.csv\")\n",
        "# init dataloader\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=100, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, vocab_size, embeding_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # implement embedding function to convert text to numerical data\n",
        "        self.embedding = nn.Embedding(vocab_size, embeding_dim)\n",
        "        self.rnn = nn.LSTM(embeding_dim, hidden_dim, num_layers =2, bidirectional=True, dropout = 0.5)\n",
        "        self.fc = nn.Linear(hidden_dim*2,1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    def forward(self, x):\n",
        "        embediding = self.dropout(self.embedding(x))\n",
        "        output,(hidden, cell) = self.rnn(embediding)\n",
        "        hidden = torch.cat([hidden[-2], hidden[-1]],dim=1)\n",
        "        hidden = self.dropout(hidden)\n",
        "        out = self.fc(hidden)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "pretrained = torchtext.vocab.GloVe()\n",
        "# 这块我也不知道怎么处理， 但本质上就是里面有个nn.embedding layer， 应该用别人已经算好的Glove来替代nn中本身自带的， 但没办法替换不了"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeding_size = 300\n",
        "vocab_size = 10000\n",
        "# init Lstm model \n",
        "BLSTM = Network(vocab_size, embeding_size, 256)\n",
        "# replace the embedding to the pretrained embedding Glove\n",
        "# 这个写法是可以的， 但问题是Glove的数据集有2.2G， 这些玩意不加筛选全部加载进内存，会导致直接爆内存，well 如果你们有什么64g内存 4090显卡这种， 那咱就不用考虑了， 直接算就完事了\n",
        "# 所以这个玩意应该先iterate一遍，只选择我们数据中出现的单词， 不用的就不用管， 但就这步反正搞不来\n",
        "# pretrained_embedding = torchtext.vocab.GloVe().vectors\n",
        "# BLSTM.embedding.weight.data.copy_(pretrained_embedding)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(BLSTM.parameters(), lr=1e-3)\n",
        "criteon = nn.BCEWithLogitsLoss().to(device)\n",
        "Network.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training process\n",
        "# 没写完\n",
        "def train(lstm, dataloader, optimizer, criteon):\n",
        "\tavg_acc = []\n",
        "\tNetwork.train()\n",
        "\tfor i, batch in enumerate(dataloader):\n",
        "\t\tpredict = lstm(dataloader.)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOMdefZJbtrcS+AakPmNnpu",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
