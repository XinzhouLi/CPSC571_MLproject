{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XinzhouLi/Toxic_Language_Detection_in_Social_Media/blob/main/LSTM_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgQz6LomhSUY",
        "outputId": "82b83459-a0b1-4dcb-9f86-d3c70db2f0c6"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells requires notebook and jupyter package.\n",
            "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
            "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
            "\u001b[1;31mor\n",
            "\u001b[1;31mconda install jupyter notebook -U'\n",
            "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/XinzhouLi/Toxic_Language_Detection_in_Social_Media.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TOjKmw8hSUd"
      },
      "source": [
        " https://www.cnblogs.com/cxq1126/p/13466998.html\n",
        " 这篇文章 有讲述如何怎么建立完整的训练过程\n",
        "\n",
        "\n",
        " https://www.cnblogs.com/cxq1126/p/13504437.html\n",
        " 这个就是我们的最终目标 LSTM加上Attention\n",
        "\n",
        "\n",
        " https://www.cnblogs.com/cxq1126/p/13508696.html\n",
        " 这个是transformer的文本分类我还没看先扔这\n",
        "\n",
        "\n",
        "我相信都接触过colab吧， 因为需要大量运算基本都不在本地跑，全都白嫖Google的算力， \n",
        "然后上面那一行是先把文件克隆到自己的drive里面这样就省的自己上传文件了\n",
        "然后你们可以先看看， 我基本都写了comment， 有一些没写的十有八九我也是抄的\n",
        "所以我也不一定知道那些代码是干嘛的， 也试试我的那些variable里面都存了啥， 每一行改一改试试，看看都是干嘛的。等之后见面开会再一起写写\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yr7kCVmDXkwX"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/johnnyzhang/Documents/GitHub/CPSC571_MLproject/LSTM_model.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/johnnyzhang/Documents/GitHub/CPSC571_MLproject/LSTM_model.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johnnyzhang/Documents/GitHub/CPSC571_MLproject/LSTM_model.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/johnnyzhang/Documents/GitHub/CPSC571_MLproject/LSTM_model.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils import data\n",
        "from torch import nn, optim\n",
        "import torchtext\n",
        "import re\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txQohGaDhSUf",
        "outputId": "f36047b3-6e8f-4be4-8e17-c4a7bd73191d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.1+cu116\n",
            "3.8.10 (default, Nov 14 2022, 12:59:47) \n",
            "[GCC 9.4.0]\n",
            "0.14.1\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)\n",
        "print(sys.version)\n",
        "print(torchtext.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zcGlcRhFhSUh"
      },
      "outputs": [],
      "source": [
        "def unify_format(text):\n",
        "    return re.sub(r\"[^a-zA-Z0-9]\", \" \", text).lower().split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VNdvyUrghSUh"
      },
      "outputs": [],
      "source": [
        "# implement dataset extends util.data.Dataset\n",
        "class ToxicDataset(data.Dataset):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        # Read data in\n",
        "        if len(args) == 1:\n",
        "            # for initialize training dataset\n",
        "            filepath = args[0]\n",
        "            self.dataframe = pd.read_csv(\n",
        "                filepath, iterator=True, header=0, encoding='utf-8', usecols=['comment_text', 'toxic'])\n",
        "            # initialize pandas dataframe to store the data\n",
        "            self.dataframe = pd.DataFrame(self.dataframe.read())\n",
        "            self.dataframe.convert_dtypes()\n",
        "            # Clean the data using regular expersion, only reserve letter and number\n",
        "            self.dataframe['comment_text'] = self.dataframe['comment_text'].apply(\n",
        "                unify_format)\n",
        "        else:\n",
        "            # for initialize testing dataset\n",
        "            filepath1 = args[0]\n",
        "            filepath2 = args[1]\n",
        "            test_comments = pd.DataFrame(pd.read_csv(\n",
        "                filepath1, iterator=True, header=0, usecols=['comment_text', 'id']).read())\n",
        "            test_label = pd.DataFrame(pd.read_csv(\n",
        "                filepath2, iterator=True, header=0, usecols=['id', 'toxic']).read())\n",
        "            # merge two dataframe together by unique key id\n",
        "            result = pd.merge(test_comments, test_label, how='left', on=['id'])\n",
        "            # select the text and label col\n",
        "            self.dataframe = result[result['toxic']>= 0].loc[:, ['comment_text', 'toxic']]\n",
        "            self.dataframe.convert_dtypes()\n",
        "            # Clean the data using regular expersion, only reserve letter and number\n",
        "            self.dataframe['comment_text'] = self.dataframe['comment_text'].apply(unify_format)\n",
        "\n",
        "    # override the getiem function, return tuple contains comment and label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        comment, label = self.dataframe.iat[index, 0], self.dataframe.iat[index, 1]\n",
        "        return comment, label\n",
        "\n",
        "    # Override the len function, return the length of the dataframe\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe.loc[:, ['comment_text']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UaTsO5PBhSUj"
      },
      "outputs": [],
      "source": [
        "# init dataset\n",
        "# train_dataset = ToxicDataset(\"train.csv\")\n",
        "# test_dataset = ToxicDataset(\"test.csv\", \"test_labels.csv\")\n",
        "train_dataset = ToxicDataset(\"/content/Toxic_Language_Detection_in_Social_Media/train.csv\")\n",
        "test_dataset = ToxicDataset(\"/content/Toxic_Language_Detection_in_Social_Media/test.csv\", \"/content/Toxic_Language_Detection_in_Social_Media/test_labels.csv\")\n",
        "# init dataloader\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=100, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DX2dghJehSUl"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, vocab_size, embeding_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # implement embedding function to convert text to numerical data\n",
        "        self.embedding = nn.Embedding(vocab_size, embeding_dim)\n",
        "        self.rnn = nn.LSTM(embeding_dim, hidden_dim, num_layers =2, bidirectional=True, dropout = 0.5)\n",
        "        self.fc = nn.Linear(hidden_dim*2,1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    def forward(self, x):\n",
        "        embediding = self.dropout(self.embedding(x))\n",
        "        output,(hidden, cell) = self.rnn(embediding)\n",
        "        hidden = torch.cat([hidden[-2], hidden[-1]],dim=1)\n",
        "        hidden = self.dropout(hidden)\n",
        "        out = self.fc(hidden)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH7cZu5PhSUm",
        "outputId": "fc17116d-fed9-42c5-deb4-2f0a30e374e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.840B.300d.zip: 2.18GB [08:24, 4.31MB/s]                            \n",
            "100%|█████████▉| 2196016/2196017 [03:36<00:00, 10131.72it/s]\n"
          ]
        }
      ],
      "source": [
        "pretrained = torchtext.vocab.GloVe()\n",
        "# 这块我也不知道怎么处理， 但本质上就是里面有个nn.embedding layer， 应该用别人已经算好的Glove来替代nn中本身自带的， 但没办法替换不了"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pchnepb4hSUn",
        "outputId": "ce416f07-c56e-44a1-90a2-b02af785e472"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Network(\n",
              "  (embedding): Embedding(2196017, 300)\n",
              "  (rnn): LSTM(300, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeding_size = 300\n",
        "vocab_size = 10000\n",
        "# init Lstm model \n",
        "BLSTM = Network(vocab_size, embeding_size, 256)\n",
        "# replace the embedding to the pretrained embedding Glove\n",
        "# 这个写法是可以的， 但问题是Glove的数据集有2.2G， 这些玩意不加筛选全部加载进内存，会导致直接爆内存，well 如果你们有什么64g内存 4090显卡这种， 那咱就不用考虑了， 直接算就完事了\n",
        "# 所以这个玩意应该先iterate一遍，只选择我们数据中出现的单词， 不用的就不用管， 但就这步反正搞不来\n",
        "# pretrained_embedding = torchtext.vocab.GloVe().vectors\n",
        "BLSTM.embedding = torch.nn.Embedding.from_pretrained(pretrained.vectors,freeze=True) \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(BLSTM.parameters(), lr=1e-3)\n",
        "criteon = nn.BCEWithLogitsLoss().to(device)\n",
        "BLSTM.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "syhrAX86hSUo",
        "outputId": "44f477da-ed82-4aa7-9981-a88cfac71a04"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-149e6f6e8ba6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'avg acc:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-149e6f6e8ba6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rnn, iterator, optimizer, criteon)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# [seq, b] => [b, 1] => [b]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0melem_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# It may be accessed twice, so we use a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
          ]
        }
      ],
      "source": [
        "# training process\n",
        "# 没写完\n",
        "def binary_acc(preds, y):\n",
        "    \"\"\"\n",
        "    get accuracy\n",
        "    \"\"\"\n",
        "    preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = torch.eq(preds, y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "def train(rnn, iterator, optimizer, criteon):\n",
        "    \n",
        "    avg_acc = []\n",
        "    rnn.train()\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        # [seq, b] => [b, 1] => [b]\n",
        "        pred = rnn(batch.text).squeeze(1)\n",
        "        # \n",
        "        loss = criteon(pred, batch.label)\n",
        "        acc = binary_acc(pred, batch.label).item()\n",
        "        avg_acc.append(acc)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i%10 == 0:\n",
        "            print(i, acc)\n",
        "        \n",
        "    avg_acc = np.array(avg_acc).mean()\n",
        "    print('avg acc:', avg_acc)\t\t\n",
        "\n",
        "train(BLSTM, train_dataloader, optimizer, criteon)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "742d48b62fc2d44a1b4c1951982374d7d122707d3771ab69e089c7dc09958407"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
